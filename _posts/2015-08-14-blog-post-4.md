---
title: 'Blog 4 - Causal Interventions'
date: 2025-01-07
permalink: /posts/2014/08/blog-post-3/
# tags:
#   - cool posts
#   - category1
#   - category2
---

## ***Why this blog?***
This blog is a reference to various casual interventions techniques. Causal intervention techniques are powerful tools in mechanistic interpretability for understanding how components of a machine learning model contribute to its overall behavior. In the context of large language models (LLMs) and neural networks, these techniques aim to reveal causal relationships between internal components (like neurons, SAE activations, attention heads, or layers) and the model's output.

## ***Introduction***
The paper [Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small](https://arxiv.org/abs/2211.00593) introduced how to perform model analysis to reverse engineer a circuit performing a particular task, specifically the IOI (Indirect Object Indetification) task. In this paper they describe how they used some causal interventions to understand the behavious of the model components while perform this task. The techniques that I found quite easy and interesting to study are given below. 

## ***Methods***
1) ***Ablation*** - An ablation is a simple causal intervention on a model - we pick the activation of some intermediate component (neuron, head, SAE activation, or layer) of a model and set it to zero. This is a crude proxy for how much that part matters. Further, if we have some story about how a specific circuit in the model enables some capability, showing that ablating other parts does nothing can be strong evidence of this. Pruning is something similar to the idea of ablation but quite different in interpretability terms. Pruning refers to setting the model weights of a particular component to zero. Also zero ablation seems unprincipled and naive because the model might use those activations as a bias, for example a particular activation value can always be between say 1000 and 1001, setting it to zero doesn't make sense since it wil break the reuslt over all tasks (and not only our task). <br>
Mean Ablation seems like a much more sensible and reasonable approach where we replace the model's component's activation with a value averaged over many instances of different tasks (or even over a task unbiased dataset, i.e raw data). This might break the model's performance over other tasks too in several cases though because technically we can expect the mean of activations over a large batch of dataset will be very close to zero for several components. <br>
Another approach which can be taken is Random Ablation or Resampling, where we replace the activation with the activation of a random data point in the dataset. This data point can be sampled from any training distribution, but can be sampled from a synthetic unbiased distribution as well. However using ablations to study and infer the model's behaviour is a bit of a stretch in models trained using Dropout, since dropout automatically applies zero ablations to random activations at training time, so it trains the model to be robust to this kind of intervention. 

2) ***Direct Logit Attribution*** - The easiest part of the model to understand is the output - this is what the model is trained to optimize, and so it can always be directly interpreted! Often the right approach to reverse engineering a circuit is to start at the end, understand how the model produces the right answer, and to then work backwards. The main technique used to do this is called direct logit attribution. The logits of a model are `logits=Unembed(LayerNorm(final_residual_stream))`. The Unembed is a linear map, and LayerNorm is approximately a linear map, so we can decompose the logits into the sum of the contributions of each component, and look at which components contribute the most to the logit of the correct token! This is called direct logit attribution. Here we look at the direct attribution to the logit difference. Logit difference is actually a really nice and elegant metric and is a particularly nice aspect of the setup of Indirect Object Identification. In general, there are two natural ways to interpret the model's outputs: the output logits, or the output log probabilities (or probabilities). The logits are much nicer and easier to understand, as noted above. However, the model is trained to optimize the cross-entropy loss (the average of log probability of the correct token). This means it does not directly optimize the logits, and indeed if the model adds an arbitrary constant to every logit, the log probabilities are unchanged. But, we have:
```
log_probs == logits.log_softmax(dim=-1) == logits - logsumexp(logits)
```
and because they differ only by a constant, their difference will be the same, i.e 
```
log_probs(" Mary") - log_probs(" John") = logits(" Mary") - logits(" John")
```

Further, the metric helps us isolate the precise capability we care about - figuring out which name is the Indirect Object. There are many other components of the task - deciding whether to return an article (the) or pronoun (her) or name, realising that the sentence wants a person next at all, etc. By taking the logit difference we control for all of that.

Our metric is further refined, because each prompt is repeated twice, for each possible indirect object. This controls for irrelevant behaviour such as the model learning that John is a more frequent token than Mary (this actually happens! The final layernorm bias increases the John logit by 1 relative to the Mary logit). Another way to handle this would be to use a large enough dataset (with names randomly chosen) that this effect is averaged out.

***Ignoring LayerNorm*** - LayerNorm is an analogous normalization technique to BatchNorm (that's friendlier to massive parallelization) that transformers use. Every time a transformer layer reads information from the residual stream, it applies a LayerNorm to normalize the vector at each position (translating to set the mean to 0 and scaling to set the variance to 1) and then applying a learned vector of weights and biases to scale and translate the normalized vector. This is almost a linear map, apart from the scaling step, because that divides by the norm of the vector and the norm is not a linear function. (The `fold_ln` flag when loading a model factors out all the linear parts). <br>
But if we fixed the scale factor, the LayerNorm would be fully linear. And the scale of the residual stream is a global property that's a function of all components of the stream, while in practice there is normally just a few directions relevant to any particular component, so in practice this is an acceptable approximation. So when doing direct logit attribution we use the `apply_ln` flag on the cache to apply the global layernorm scaling factor to each constant. See my clean GPT-2 implementation for more on LayerNorm.


Getting an output logit is equivalent to projecting onto a direction in the residual stream, and the same is true for getting the logit diff. <br>

Suppose our final value in the residual stream for a single sequence and a position within that sequence is \\(x\\) (i.e \\(x\\) is a vector of length \\(d_{model}\\)). Then if we ignore layer norm, the logits are given by multiplying the resiudal stream with the Unembedding matrix (\\(W_U\\)) which has shape (\\((d_{model}, d_{vocab})\\)): <br>
Output Logits = \\(x^{T}W_{U}\\) <br>
Since we want to evaluate the logit difference between the indirect object and the subject, (which as mentioned above is a reasonable criterion and similar to evaluating the difference between the output probabilities) we can write it as following: <br>
Logit diff \\(= output_{io} - output_{s}\\) <br>
Logit diff \\(= {x^{T}W_{U}}_{io} - {x^{T}W_{U}}_{s}\\) <br>
Logit diff \\(= x^{T}(u_{IO} - u_{S})\\) <br>
where, \\(u_{io}\\) and \\(u_{s}\\) are the columns of the Unembedding matrix corresponding to the indirect object and the subject tokens respectively.  

To summarize, we've written the logit diff as a dot product between the vector in the residual stream and a constant vector (which is a function of the model's unembedding matrix). We call this vector \\(u_{io} - u_{s}\\) as the logit difference direction vector.   

3) ***Activation Patching*** - The setup of activation pactching is such that we run the model on two different inputs, the clean input and the corrected input. The clean input is the one where the model generates the correct answer (or the answer that we seem/assume to be correct), whereas the corrupted input is the run where the model generates the incorrect answer. We can broadly classify Activation Patching into two main alogrithms - noising and denoising.
    - ***Noising*** -  In noising, we take the activation from the corrupted run of some component of the model (neuron, residual layer, attention head, or MLP) and use it by replacing the activation of the same component of the model for the clean run. 
    - ***Denoising*** - In denoising, we take the activation from the clean run of some component of the model (neuron, residual layer, attention head, or MLP) and use it by replacing the activation of the same component of the model for the corrupted run.

An easy to interpret visualization of Activation Patching:
![Activaion Patching](/images/activation_patching.png)

An example of Activation Patching for the Indirect Object Identification (IOI) task: Assume: <br>
Clean Input -> When `Mary` and `John` went to the shop, `John` gave the bag to <br>
Corrupted Input -> When `Mary` and `John` went to the shop, `Mary` gave the bag to <br>

***Note*** - We could reverse all the three names for our corrupted run but we refrain from doing so. <br>
Alternate Corrupt Run: When `John` and `Mary` went to the shop, `Mary` gave the bag to <br>

We want to understand how the model performs the IOI task, but if we use the alternate corrupted run, the model might learn that the correct next token (or the next token to be predicted) is always the token 2nd token (In our clean input case, it is `Mary` and in our alternate corrupted input it is `John`). That is the model will use the information that is directed to the position of the token instead of the semantic meaning of the token and thus the task.

4) ***Path Patching*** - Path Patching is ore of an extension to Activation Patching and the intuition here is quite similar. The point is that we try to make sure that every direct path from the `Sender` to the `Receiver` is from the corrupted run (i.e we patch not an activation but a whole path) and every indirect path between the `Sender` and the `Receiver` is from the clean run. In simpler words, we patch the direct path between the `Sender` and the `Receiver` (and thus the name "Path Patching"). Here we define a path which consists of a `Sender` node and a `Receiver` node. We patch the activations of the `Sender` node from the corrupted run and let it affect only the `Receiver` node and not any other node unlike in Activation Patching. Say we have two components in a circuit Head A (`Sender`) and Head B (`Reciever`), if we want to understand the circuit, we might want to understand what happens when the activation coming from the Head A to the Head B instead comes from a different distribution (i.e the corrupted run) while keeping all other incoming activations to the Head B and outgoing activations of the Head A frozen as in the clean run. Now cache this value for the Head B and patch this in a clean run. This gives us a very good basis of if these two components really form a circuit. SUmmarizing our three step process:
    - Cache values of the clean and corrupted distribution.
    - Patch the activation of the `Sender`, freeze everything else, cache the activation of the `Receiver`.
    - Patch this activation of the `Reciever` in a clean run to obtain the logits.              

An easy to interpret visualization of Path Patching:
![Path Patching](/images/path_patching.png)

Let's make this concrete, and take a simple 3-layer transformer with 2 heads per layer. Let's perform path patching on the edge from head `0.0` to `2.0` (terminology note: `0.0` is the `Sender`, and `2.0` is the `Receiver`). Note that here, we're considering "direct paths" as anything that doesn't go through another attention head (so it can go through any combination of MLPs). Intuitively, the nodes (attention heads) are the only things that can move information around in the model, and this is the thing we want to study. In contrast, MLPs just perform information processing, and they're not as interesting for this task. Our 3-step process looks like the diagram below (remember green is corrupted, grey is clean). 
![Path Patching in a transformer](/images/path_patching_transformer.png)





