---
title: 'Blog 3 - Superposition'
date: 2024-12-16
permalink: /posts/2014/08/blog-post-3/
# tags:
#   - cool posts
#   - category1
#   - category2
---

## ***Why this blog?***
This blog is a reference to some core concepts of Superposition Principle in Large Language Models (LLMs). A large part of this blog post is inspired from the following sources:
- [Anthropic's - Toy Models of Superposition paper](https://transformer-circuits.pub/2022/toy_model/index.html) 
- [Neel Nanda's Dynalist notes](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=3br1psLRIjQCOv2T4RN3V6F2)

## ***Introduction***
In Mechanistic Intepretability, it is often assumed of neural networks as having features of the input represented as directions in activation space. e. When we say something like "word embeddings have a gender direction" or "vision models have curve detector neurons", one is implicitly making strong claims about the structure of network representations. This non-trivial assumption is termed as Linear Representation Hypothesis. I like to define or think of this as an idea that high level concepts are represented linearly in the activation space of the model. We can think of this hypothesis as a result of 2 seperate properties:
- Decomposability - Network representations can be described in terms of independently understandable features.
- Linearity - Independently understandable features are represented by direction.

Sometimes, identifying feature directions (directions that represent independent understandable features) is very easy because features seem to correspond to neurons. Why is it that we sometimes get this extremely helpful property, but in other cases don't? The paper (linked above) hypothesizes that there are really two countervailing forces driving this:
- Privileged Basis - Only some representations have a privileged basis which encourages features to align with basis directions (i.e. to correspond to neurons).
- Superposition: Linear representations can represent more features than dimensions, using a strategy we call superposition. This can be seen as neural networks simulating larger networks. This pushes features away from corresponding to neurons.

Before moving to the idea of superposition, let us explore what are priveliged and non-priveliged bases.

## Priveliged and Non-Priveliged Bases
<p>A <a href="https://en.m.wikipedia.org/wiki/Basis_(linear_algebra)">basis</a> is the set of building-block vectors which we can use to represent any other vector in a particular space.</p>
<p>Basis vectors should be <em>independent</em> of one another, ie we should not be able to express any of the vectors in the set using a combination of the others.</p>
<p>For example, in a 2D vector space, the vectors <code>[1, 0]</code> and <code>[0, 1]</code> form a basis, because any other vector in the space can be expressed as a linear combination of them, and it's not possible to express one with the other.</p>
<p>For example, the vector <code>[2, 3]</code> can be represented as:</p>
<p><code>[2, 3] = 2[1, 0] + 3[0, 1]</code></p>
<p>The coefficients 2 and 3 represent how much of each basis vector is needed to form the original vector.</p>
<p>We could also use the vectors <code>[1, 1]</code> and <code>[-1, 1]</code> form an alternative 2D basis, allowing us to express the original vector differently:</p>
<p><code>[2, 3] = 2[1, 1] + (-1)[-1, 1]</code></p>
<p>Bases are important because they allow us to represent complex data, like the features of images or text, as collections of simpler components. The choice of basis can have a significant impact on the ability of an algorithm to learn patterns and relationships within the data. By choosing an appropriate basis, we can simplify the representation of the data and make it easier for an algorithm to extract meaningful features.</p>
<h3>Privileged bases</h3>
<p>In certain problem settings, we can design models with architectural choices that deliberately bias them to structure their internal representation of features along specific basis dimensions. These biases are commonly referred to as <a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive biases</a>. When such dimensions (or groups of dimensions) acquire special significance during training, we call them <em>privileged</em> bases.

For instance, in a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a> (CNN), the convolutional layers are explicitly constructed to capture local spatial patterns in images, such as edges, corners, and textures. By constraining neurons to focus on adjacent regions of the input, these layers encourage the model to learn spatially localised features that naturally align with the basis dimensions.

Take the example of a CNN trained to classify images of cats and dogs. If the network’s learned features fail to align with basis dimensions that correspond to distinguishing traits, such as whiskers or ears, it might struggle to differentiate between the two classes. Conversely, when the learned features are well-aligned with these dimensions, the network can effectively capture meaningful patterns in the input, leading to higher classification accuracy.

Similarly, the <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">multi-head attention</a> mechanism in a <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer model</a> is designed to enable different heads to focus on distinct aspects of the input sequence. This design encourages diverse feature alignment across the heads, facilitating better learning of complex patterns.

Additionally, <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularisation techniques</a>, such as weight decay, dropout, or batch normalisation, introduce constraints into the learning process. These constraints reduce overfitting or control model complexity, implicitly nudging the model to learn features that are better aligned with the basis dimensions. Such constraints act as priors, guiding the model to develop representations that are both efficient and interpretable.

</p>
<h3>Non-privileged bases</h3>
<p>In the creation of a non-privileged basis, the model is given no access to privileged information, and the optimiser is given no incentive to align its basis dimensions with a particular type of feature. Instead, the available dimensions are treated as equally important, and the model is free to distribute information equally across them.</p>
<p>For example, in an embedding space generated by a process like <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a>, each word is represented as a high-dimensional vector, with each dimension corresponding to a learned feature of the word. Rather than being deliberately engineered to represent specific linguistic features, the basis vectors are distributed based solely on the relationships of words to one other in the space, regardless of how that vector space itself is arranged.</p>
<p>The distribution of basis vectors in the space is driven wholly by the optimiser, rather than by engineering specific dimensions to capture specific aspects of the data.</p>
<p>In cases where the space is larger than the information being encoded, this can result in information being represented redundantly and non-orthogonally across many dimensions. This can make model with non-privileged bases more robust to noise and variations, at the expense of interpretability.</p>
<h3>Rotation</h3>
<p>Apart from considering the priors in an algorithm's architecture or its training data, the easiest way to determine whether a basis is privileged is to consider the effect of a <a href="https://en.m.wikipedia.org/wiki/Rotation_(mathematics)">rotation</a> on algorithmic performance.</p>
<p>In a non-privileged basis, rotation should have no impact on the effectiveness of the vectors in a model, because their features are not aligned with any specific basis dimension.</p>
<p>In contrast, in a privileged basis, such as in a CNN or transformer MLP, the basis dimensions are designed to capture specific types of features, and there is an incentive for the learned features to align with these dimensions. Rotating the basis would change the orientation of the basis dimensions relative to the learned features. In almost all cases, this misalignment will degrade the model's performance.</p>
<p>For example, a tiger-or-not-tiger model might contain a filter which is trained to capture orange stripes in the input data. Rotating this filter to detect <em>blue</em> stripes instead would make this feature much less useful!</p>

## Monosemanticity and Polysemanticity
A feature or neuron is monosemantic if it represents exactly one concept or interpretable feature. In monosemantic neurons, the feature aligns well with a privileged basis, meaning the model’s representation naturally incentivizes disentangling and isolating that specific feature during training. In an image classification network, a neuron might activate only for vertical edges. This alignment with a spatial basis (e.g., localized patterns) makes the feature clean and interpretable.

A feature or neuron is polysemantic if it represents multiple unrelated concepts or features, depending on the context. Polysemanticity is something we would expect to observe if features were not aligned with a neuron despite having the incentive to align with the priveliged basis. A neuron in a language model might activate for both the grammatical structure of past tense and a semantic concept like "dogs" because of shared correlations in training data.

## Superposition
Superposition is when a model represents more than n-features in an n-dimensional activation space. That is, features still correspond to directions, but the set of interpretable directions is larger than the number of dimensions. Intuitively, this is the model simulating a larger model. In the superposition hypothesis, features can't align with the basis because the model embeds more features than there are neurons. Polysemanticity is inevitable if this occurs. Thus it is right to say that Superposition -> Polysemanticity but not the other way round. Several results from mathematics suggest that Superposition might be possible:
- ***Almost Orthogonal Vectors*** - Although it's only possible to have n orthogonal vectors in an n-dimensional, it is possible to have exp(n) many "almost orthoganal" vectors (with $< \epsilon$ cosine similarity) in high-dimensional spaces.
- ***Compressed sensing*** - In general, if one projects a vector into a lower-dimensional space, one can't reconstruct the original vector. However, this changes if one knows that the original vector is sparse. In this case, it is often possible to recover the original vector.

Sparse features refer to features that are active or relevant only in a small number of contexts or situations. They have low activation frequency in the model's representation space. For instance:
- A sparse feature might only activate when a specific shape or pattern appears in an image.
- In text, it could correspond to a rare grammatical structure or a seldom-used word.

Dense Features refer to features that are frequently active and represent common patterns. In superposition, dense features are more likely to overlap, leading to entanglement and potential ambiguity in the model's representation.

Some examples of these in the Image Domain:
- A dense feature could be a common edge detector that activates for any edge in an image.
- A sparse feature might activate only for specific textures, such as tiger stripes, which appear in a small subset of the training data.

Some examples of these in the Language Domain:
- A dense feature could encode common grammatical structures, like subject-verb-object relationships.
- A sparse feature might correspond to the use of a rare idiomatic expression or word.
